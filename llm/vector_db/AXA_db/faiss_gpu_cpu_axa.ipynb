{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'env.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.env\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\shutil.py:427\u001b[0m, in \u001b[0;36mcopy\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[0;32m    426\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[1;32m--> 427\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m copymode(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\shutil.py:264\u001b[0m, in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    262\u001b[0m     os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mreadlink(src), dst)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[0;32m    267\u001b[0m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'env.txt'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy(\"env.txt\", \".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall fitz -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install fitz frontend pymupdf pypdf dotenv langchain torch langchain_community transformers datasets\n",
    "# !pip install \"numpy<2\"\n",
    "# !pip install --upgrade pybind11\n",
    "# !pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"pdf_folder_axa\"\n",
    "car_list = os.listdir(\"./pdf_folder_axa/1_자동차보험\")\n",
    "short_list = os.listdir(\"./pdf_folder_axa/2_일반보험\")\n",
    "long_list = os.listdir(\"./pdf_folder_axa/3_장기보험\")\n",
    "long_hurt = []# long_save = []\n",
    "long_etc = []\n",
    "for i in long_list:\n",
    "    if i == \"./pdf_folder_axa/1_자동차보험\":\n",
    "        long_etc.append(i)\n",
    "    else:\n",
    "        long_hurt.append(i)\n",
    "\n",
    "# len(file_list), len(car_list)+len(short_list)+len(long_hurt)+len(long_etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(car_list),len(short_list),len(long_hurt),len(long_save),len(long_etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# import fitz  # PyMuPDF\n",
    "import re\n",
    "\n",
    "# 1️⃣ GPU 사용 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자동차 보험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 12개의 자동차보험 PDF 문서 처리 시작...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:   8%|▊         | 1/12 [2:05:12<22:57:16, 7512.40s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS 오류] 001_개인용자동차보험.pdf: module 'faiss' has no attribute 'StandardGpuResources'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  17%|█▋        | 2/12 [3:05:30<14:30:15, 5221.50s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS 오류] 002_개인용자동차보험공동.pdf: module 'faiss' has no attribute 'StandardGpuResources'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  17%|█▋        | 2/12 [3:49:02<19:05:10, 6871.05s/파일]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 82\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), batch_size):\n\u001b[0;32m     81\u001b[0m     batch \u001b[38;5;241m=\u001b[39m texts[i : i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m---> 82\u001b[0m     batch_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(batch_embeddings)\n\u001b[0;32m     85\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(embeddings)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[1;32mIn[19], line 26\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     23\u001b[0m pool_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 26\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, pool_mask\u001b[38;5;241m=\u001b[39mpool_mask)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\kakaocorp\\kanana-nano-2.1b-embedding\\a944ebfd59bcc9a366377ff29a5b4073c5cee1fe\\modeling_kanana2vec.py:187\u001b[0m, in \u001b[0;36mKanana2VecModel.forward\u001b[1;34m(self, input_ids, attention_mask, pool_mask, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m         input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    186\u001b[0m     ):\n\u001b[1;32m--> 187\u001b[0m     last_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    191\u001b[0m     pool_mask \u001b[38;5;241m=\u001b[39m pool_mask\u001b[38;5;241m.\u001b[39mto(last_hidden_states\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    192\u001b[0m     s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(last_hidden_states \u001b[38;5;241m*\u001b[39m pool_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:594\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    583\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    584\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    591\u001b[0m         position_embeddings,\n\u001b[0;32m    592\u001b[0m     )\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 594\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    595\u001b[0m         hidden_states,\n\u001b[0;32m    596\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    597\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    598\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    599\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    600\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    601\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    602\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    604\u001b[0m     )\n\u001b[0;32m    606\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:352\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    351\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 352\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    355\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:190\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 190\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# 경로 설정\n",
    "base_path = \"./pdf_folder_axa/\"\n",
    "car_path = os.path.join(base_path, \"1_자동차보험\")\n",
    "car_list = os.listdir(car_path)\n",
    "\n",
    "# 저장 폴더 생성\n",
    "os.makedirs(\"faiss_index\", exist_ok=True)\n",
    "os.makedirs(\"metadata\", exist_ok=True)\n",
    "\n",
    "print(f\"총 {len(car_list)}개의 자동차보험 PDF 문서 처리 시작...\\n\")\n",
    "\n",
    "for file in tqdm(car_list, desc=\"자동차 PDF 처리 중\", unit=\"파일\"):\n",
    "    file_path = os.path.join(car_path, file)\n",
    "    base_filename = os.path.splitext(file)[0]\n",
    "\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except Exception as e:\n",
    "        print(f\"[오류] {file} 처리 중 에러 발생: {e}\")\n",
    "        continue\n",
    "\n",
    "    all_texts = []\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)\n",
    "\n",
    "    total_text = \"\\n\".join(all_texts)\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))\n",
    "\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)\n",
    "\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "    embeddings = []\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "    try:\n",
    "        res = faiss.StandardGpuResources()\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)\n",
    "        gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "        gpu_index.add(embeddings)\n",
    "\n",
    "        faiss_path = f\"faiss_index/{base_filename}.faiss\"\n",
    "        faiss.write_index(faiss.index_gpu_to_cpu(gpu_index), faiss_path)\n",
    "        print(f\"{file} → FAISS 저장 완료: {faiss_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAISS 오류] {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    metadata_path = f\"metadata/{base_filename}.pkl\"\n",
    "    with open(metadata_path, \"wb\") as f:\n",
    "        pickle.dump(split_documents, f)\n",
    "        print(f\"{file} → 메타데이터 저장 완료: {metadata_path}\")\n",
    "\n",
    "print(\"\\n모든 자동차보험 PDF 처리 및 벡터 DB 저장이 완료되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 12개의 자동차보험 PDF 문서 처리 시작...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:   8%|▊         | 1/12 [1:28:13<16:10:25, 5293.20s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS 오류] 001_개인용자동차보험.pdf: Error in __cdecl faiss::FileIOWriter::FileIOWriter(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:102: Error: 'f' failed: could not open faiss_index/001_개인용자동차보험.faiss for writing: Illegal byte sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  17%|█▋        | 2/12 [2:37:46<12:52:25, 4634.51s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS 오류] 002_개인용자동차보험공동.pdf: Error in __cdecl faiss::FileIOWriter::FileIOWriter(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:102: Error: 'f' failed: could not open faiss_index/002_개인용자동차보험공동.faiss for writing: Illegal byte sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  25%|██▌       | 3/12 [16:22:51<62:48:28, 25123.18s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS 오류] 003_다이렉트개인용자동차보험.pdf: Error in __cdecl faiss::FileIOWriter::FileIOWriter(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:102: Error: 'f' failed: could not open faiss_index/003_다이렉트개인용자동차보험.faiss for writing: Illegal byte sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  33%|███▎      | 4/12 [17:56:16<38:42:21, 17417.72s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS 오류] 004_다이렉트개인용인터넷자동차보험.pdf: Error in __cdecl faiss::FileIOWriter::FileIOWriter(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:102: Error: 'f' failed: could not open faiss_index/004_다이렉트개인용인터넷자동차보험.faiss for writing: Illegal byte sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  33%|███▎      | 4/12 [18:14:46<36:29:33, 16421.69s/파일]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 86\u001b[0m\n\u001b[0;32m     84\u001b[0m batch \u001b[38;5;241m=\u001b[39m texts[i : i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(emb)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[20], line 33\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     30\u001b[0m pool_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 33\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, pool_mask\u001b[38;5;241m=\u001b[39mpool_mask)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\kakaocorp\\kanana-nano-2.1b-embedding\\a944ebfd59bcc9a366377ff29a5b4073c5cee1fe\\modeling_kanana2vec.py:187\u001b[0m, in \u001b[0;36mKanana2VecModel.forward\u001b[1;34m(self, input_ids, attention_mask, pool_mask, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m         input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    186\u001b[0m     ):\n\u001b[1;32m--> 187\u001b[0m     last_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    191\u001b[0m     pool_mask \u001b[38;5;241m=\u001b[39m pool_mask\u001b[38;5;241m.\u001b[39mto(last_hidden_states\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    192\u001b[0m     s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(last_hidden_states \u001b[38;5;241m*\u001b[39m pool_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:594\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    583\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    584\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    591\u001b[0m         position_embeddings,\n\u001b[0;32m    592\u001b[0m     )\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 594\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    595\u001b[0m         hidden_states,\n\u001b[0;32m    596\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    597\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    598\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    599\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    600\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    601\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    602\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    604\u001b[0m     )\n\u001b[0;32m    606\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:352\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    351\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 352\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    355\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:190\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 190\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# ✅ 모델 및 토크나이저 로드\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# ✅ 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    pool_mask = torch.ones_like(inputs[\"input_ids\"]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# ✅ 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# ✅ 경로 설정\n",
    "base_path = \"./pdf_folder_axa/1_자동차보험\"\n",
    "car_list = os.listdir(base_path)\n",
    "\n",
    "# ✅ 저장 폴더 생성\n",
    "os.makedirs(\"faiss_index\", exist_ok=True)\n",
    "os.makedirs(\"metadata\", exist_ok=True)\n",
    "\n",
    "print(f\"총 {len(car_list)}개의 자동차보험 PDF 문서 처리 시작...\\n\")\n",
    "\n",
    "for file in tqdm(car_list, desc=\"자동차 PDF 처리 중\", unit=\"파일\"):\n",
    "    file_path = os.path.join(base_path, file)\n",
    "    base_filename = os.path.splitext(file)[0]\n",
    "\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except Exception as e:\n",
    "        print(f\"[오류] {file} 처리 중: {e}\")\n",
    "        continue\n",
    "\n",
    "    # ✅ 텍스트 정제 및 문서화\n",
    "    all_texts = [clean_text(doc.page_content) for doc in documents]\n",
    "    total_text = \"\\n\".join(all_texts)\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))\n",
    "\n",
    "    new_documents = [\n",
    "        Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        for i, text in enumerate(unique_texts) if text.strip()\n",
    "    ]\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)\n",
    "\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "    embeddings = []\n",
    "    batch_size = 16\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        try:\n",
    "            emb = get_embeddings(batch)\n",
    "            embeddings.append(emb)\n",
    "        except Exception as e:\n",
    "            print(f\"[임베딩 오류] {file} 배치 실패: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not embeddings:\n",
    "        print(f\"[건너뜀] {file} → 유효한 임베딩 없음\")\n",
    "        continue\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "    # ✅ FAISS 저장 (CPU용)\n",
    "    try:\n",
    "        dim = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "        index.add(embeddings)\n",
    "\n",
    "        faiss_path = f\"faiss_index/{base_filename}.faiss\"\n",
    "        faiss.write_index(index, faiss_path)\n",
    "        print(f\"{file} → FAISS 저장 완료: {faiss_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAISS 오류] {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # ✅ 메타데이터 저장\n",
    "    metadata_path = f\"metadata/{base_filename}.pkl\"\n",
    "    with open(metadata_path, \"wb\") as f:\n",
    "        pickle.dump(split_documents, f)\n",
    "        print(f\"{file} → 메타데이터 저장 완료: {metadata_path}\")\n",
    "\n",
    "print(\"\\n✅ 모든 PDF 문서 처리 및 벡터 저장이 완료되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 12개의 자동차보험 PDF 문서 처리 시작...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:   8%|▊         | 1/12 [1:50:15<20:12:52, 6615.71s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS 오류] 001_개인용자동차보험.pdf: Error in __cdecl faiss::FileIOWriter::FileIOWriter(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:102: Error: 'f' failed: could not open faiss_index/001_개인용자동차보험.faiss for writing: Illegal byte sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:   8%|▊         | 1/12 [2:10:41<23:57:35, 7841.41s/파일]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 91\u001b[0m\n\u001b[0;32m     89\u001b[0m batch \u001b[38;5;241m=\u001b[39m texts[i : i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(emb)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[21], line 37\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     34\u001b[0m pool_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 37\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, pool_mask\u001b[38;5;241m=\u001b[39mpool_mask)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\kakaocorp\\kanana-nano-2.1b-embedding\\a944ebfd59bcc9a366377ff29a5b4073c5cee1fe\\modeling_kanana2vec.py:187\u001b[0m, in \u001b[0;36mKanana2VecModel.forward\u001b[1;34m(self, input_ids, attention_mask, pool_mask, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m         input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    186\u001b[0m     ):\n\u001b[1;32m--> 187\u001b[0m     last_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    191\u001b[0m     pool_mask \u001b[38;5;241m=\u001b[39m pool_mask\u001b[38;5;241m.\u001b[39mto(last_hidden_states\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    192\u001b[0m     s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(last_hidden_states \u001b[38;5;241m*\u001b[39m pool_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:594\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    583\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    584\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    591\u001b[0m         position_embeddings,\n\u001b[0;32m    592\u001b[0m     )\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 594\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    595\u001b[0m         hidden_states,\n\u001b[0;32m    596\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    597\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    598\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    599\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    600\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    601\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    602\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    604\u001b[0m     )\n\u001b[0;32m    606\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:352\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    351\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 352\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    355\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:190\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 190\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 안전한 파일명 변환 함수\n",
    "def get_safe_filename(name):\n",
    "    return ''.join(c if c.isalnum() else '_' for c in name)\n",
    "\n",
    "# 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    pool_mask = torch.ones_like(inputs[\"input_ids\"]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# 경로 설정\n",
    "base_path = \"./pdf_folder_axa/1_자동차보험\"\n",
    "car_list = os.listdir(base_path)\n",
    "\n",
    "# 저장 폴더 생성\n",
    "os.makedirs(\"faiss_index\", exist_ok=True)\n",
    "os.makedirs(\"metadata\", exist_ok=True)\n",
    "\n",
    "print(f\"총 {len(car_list)}개의 자동차보험 PDF 문서 처리 시작...\\n\")\n",
    "\n",
    "for file in tqdm(car_list, desc=\"자동차 PDF 처리 중\", unit=\"파일\"):\n",
    "    file_path = os.path.join(base_path, file)\n",
    "    base_filename = os.path.splitext(file)[0]\n",
    "    safe_filename = get_safe_filename(base_filename)\n",
    "\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except Exception as e:\n",
    "        print(f\"[오류] {file} 처리 중: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 텍스트 정제 및 문서화\n",
    "    all_texts = [clean_text(doc.page_content) for doc in documents]\n",
    "    total_text = \"\\n\".join(all_texts)\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))\n",
    "\n",
    "    new_documents = [\n",
    "        Document(page_content=text, metadata={\"page\": i + 1, \"source\": file})\n",
    "        for i, text in enumerate(unique_texts) if text.strip()\n",
    "    ]\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)\n",
    "\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "    embeddings = []\n",
    "    batch_size = 16\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        try:\n",
    "            emb = get_embeddings(batch)\n",
    "            embeddings.append(emb)\n",
    "        except Exception as e:\n",
    "            print(f\"[임베딩 오류] {file} 배치 실패: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not embeddings:\n",
    "        print(f\"[건너뜀] {file} → 유효한 임베딩 없음\")\n",
    "        continue\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "    # FAISS 저장 (CPU용)\n",
    "    try:\n",
    "        dim = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "        index.add(embeddings)\n",
    "\n",
    "        faiss_path = f\"faiss_index/{safe_filename}.faiss\"\n",
    "        faiss.write_index(index, faiss_path)\n",
    "        print(f\"{file} → FAISS 저장 완료: {faiss_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAISS 오류] {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 메타데이터 저장 (원본 파일명 유지)\n",
    "    metadata_path = f\"metadata/{safe_filename}.pkl\"\n",
    "    with open(metadata_path, \"wb\") as f:\n",
    "        pickle.dump(split_documents, f)\n",
    "        print(f\"{file} → 메타데이터 저장 완료: {metadata_path}\")\n",
    "\n",
    "print(\"\\n모든 PDF 문서 처리 및 벡터 저장이 완료되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 12개의 자동차보험 PDF 문서 처리 시작...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:   8%|▊         | 1/12 [1:36:17<17:39:10, 5777.32s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS 저장 실패] 001_개인용자동차보험.pdf: Error in __cdecl faiss::FileIOWriter::FileIOWriter(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:102: Error: 'f' failed: could not open c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\faiss_index_safe\\001_개인용자동차보험.faiss for writing: Illegal byte sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:   8%|▊         | 1/12 [1:42:48<18:50:54, 6168.58s/파일]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 95\u001b[0m\n\u001b[0;32m     93\u001b[0m batch \u001b[38;5;241m=\u001b[39m texts[i : i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(emb)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[22], line 47\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     44\u001b[0m pool_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 47\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, pool_mask\u001b[38;5;241m=\u001b[39mpool_mask)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\kakaocorp\\kanana-nano-2.1b-embedding\\a944ebfd59bcc9a366377ff29a5b4073c5cee1fe\\modeling_kanana2vec.py:187\u001b[0m, in \u001b[0;36mKanana2VecModel.forward\u001b[1;34m(self, input_ids, attention_mask, pool_mask, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m         input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    186\u001b[0m     ):\n\u001b[1;32m--> 187\u001b[0m     last_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    191\u001b[0m     pool_mask \u001b[38;5;241m=\u001b[39m pool_mask\u001b[38;5;241m.\u001b[39mto(last_hidden_states\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    192\u001b[0m     s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(last_hidden_states \u001b[38;5;241m*\u001b[39m pool_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:594\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    583\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    584\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    591\u001b[0m         position_embeddings,\n\u001b[0;32m    592\u001b[0m     )\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 594\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    595\u001b[0m         hidden_states,\n\u001b[0;32m    596\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    597\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    598\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    599\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    600\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    601\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    602\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    604\u001b[0m     )\n\u001b[0;32m    606\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:336\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    337\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    338\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    339\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    340\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    341\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    342\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    343\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    344\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    346\u001b[0m )\n\u001b[0;32m    347\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:270\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    268\u001b[0m hidden_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m--> 270\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    271\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    272\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# 안전한 파일명 생성 (영문, 숫자만)\n",
    "def get_safe_filename(name):\n",
    "    name = unicodedata.normalize('NFKD', name)\n",
    "    return ''.join(c if c.isalnum() else '_' for c in name)\n",
    "\n",
    "# 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    pool_mask = torch.ones_like(inputs[\"input_ids\"]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# 경로 설정\n",
    "base_pdf_dir = \"./pdf_folder_axa/1_자동차보험\"\n",
    "car_list = os.listdir(base_pdf_dir)\n",
    "\n",
    "# 안전한 저장 디렉토리\n",
    "faiss_dir = os.path.abspath(\"faiss_index_safe\")\n",
    "metadata_dir = os.path.abspath(\"metadata_safe\")\n",
    "os.makedirs(faiss_dir, exist_ok=True)\n",
    "os.makedirs(metadata_dir, exist_ok=True)\n",
    "\n",
    "print(f\"총 {len(car_list)}개의 자동차보험 PDF 문서 처리 시작...\\n\")\n",
    "\n",
    "for file in tqdm(car_list, desc=\"자동차 PDF 처리 중\", unit=\"파일\"):\n",
    "    file_path = os.path.join(base_pdf_dir, file)\n",
    "    base_filename = os.path.splitext(file)[0]\n",
    "    safe_filename = get_safe_filename(base_filename)\n",
    "\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except Exception as e:\n",
    "        print(f\"[오류] {file} 로드 실패: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 텍스트 정제 및 문서화\n",
    "    all_texts = [clean_text(doc.page_content) for doc in documents]\n",
    "    total_text = \"\\n\".join(all_texts)\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))\n",
    "\n",
    "    new_documents = [\n",
    "        Document(page_content=text, metadata={\"page\": i + 1, \"source\": file})\n",
    "        for i, text in enumerate(unique_texts) if text.strip()\n",
    "    ]\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)\n",
    "\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "    embeddings = []\n",
    "    batch_size = 16\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        try:\n",
    "            emb = get_embeddings(batch)\n",
    "            embeddings.append(emb)\n",
    "        except Exception as e:\n",
    "            print(f\"[임베딩 오류] {file} - 배치 {i // batch_size + 1}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not embeddings:\n",
    "        print(f\"[건너뜀] {file} → 유효한 임베딩 없음\")\n",
    "        continue\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "    # FAISS 인덱스 저장\n",
    "    try:\n",
    "        dim = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "        index.add(embeddings)\n",
    "\n",
    "        faiss_path = os.path.join(faiss_dir, f\"{safe_filename}.faiss\")\n",
    "        faiss.write_index(index, faiss_path)\n",
    "        print(f\"{file} → FAISS 저장 완료: {faiss_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAISS 저장 실패] {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 메타데이터 저장\n",
    "    try:\n",
    "        metadata_path = os.path.join(metadata_dir, f\"{safe_filename}.pkl\")\n",
    "        with open(metadata_path, \"wb\") as f:\n",
    "            pickle.dump(split_documents, f)\n",
    "        print(f\"{file} → 메타데이터 저장 완료: {metadata_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[메타데이터 저장 실패] {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n✅ 모든 PDF 문서 처리 및 벡터 저장이 완료되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 12개의 자동차보험 PDF 문서 처리 시작...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:   0%|          | 0/12 [00:00<?, ?파일/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 처리 중: 001_개인용자동차보험.pdf → 저장 파일명: 001_개인용자동차보험_08a2926e.faiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:   8%|▊         | 1/12 [1:32:04<16:52:44, 5524.04s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAISS 저장 실패] 001_개인용자동차보험.pdf: Error in __cdecl faiss::FileIOWriter::FileIOWriter(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:102: Error: 'f' failed: could not open c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\faiss_index_safe\\001_개인용자동차보험_08a2926e.faiss for writing: Illegal byte sequence\n",
      "파일 처리 중: 002_개인용자동차보험공동.pdf → 저장 파일명: 002_개인용자동차보험공동_5332a74d.faiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:   8%|▊         | 1/12 [1:37:52<17:56:33, 5872.17s/파일]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 98\u001b[0m\n\u001b[0;32m     96\u001b[0m batch \u001b[38;5;241m=\u001b[39m texts[i : i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(emb)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[23], line 48\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     45\u001b[0m pool_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 48\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, pool_mask\u001b[38;5;241m=\u001b[39mpool_mask)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\kakaocorp\\kanana-nano-2.1b-embedding\\a944ebfd59bcc9a366377ff29a5b4073c5cee1fe\\modeling_kanana2vec.py:187\u001b[0m, in \u001b[0;36mKanana2VecModel.forward\u001b[1;34m(self, input_ids, attention_mask, pool_mask, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m         input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    186\u001b[0m     ):\n\u001b[1;32m--> 187\u001b[0m     last_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    191\u001b[0m     pool_mask \u001b[38;5;241m=\u001b[39m pool_mask\u001b[38;5;241m.\u001b[39mto(last_hidden_states\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    192\u001b[0m     s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(last_hidden_states \u001b[38;5;241m*\u001b[39m pool_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:594\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    583\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    584\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    591\u001b[0m         position_embeddings,\n\u001b[0;32m    592\u001b[0m     )\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 594\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    595\u001b[0m         hidden_states,\n\u001b[0;32m    596\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    597\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    598\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    599\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    600\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    601\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    602\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    604\u001b[0m     )\n\u001b[0;32m    606\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:352\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    351\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 352\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    355\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:190\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 190\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# 완전 안전한 파일명 생성 (ASCII only + md5 해시)\n",
    "def get_safe_filename(name):\n",
    "    name_ascii = re.sub(r'[^\\w]', '_', name)  # 영문, 숫자, 밑줄만 남김\n",
    "    hash_digest = hashlib.md5(name.encode('utf-8')).hexdigest()[:8]\n",
    "    return f\"{name_ascii}_{hash_digest}\"\n",
    "\n",
    "# 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    pool_mask = torch.ones_like(inputs[\"input_ids\"]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# 경로 설정\n",
    "base_pdf_dir = \"./pdf_folder_axa/1_자동차보험\"\n",
    "car_list = os.listdir(base_pdf_dir)\n",
    "\n",
    "# 안전한 저장 디렉토리\n",
    "faiss_dir = os.path.abspath(\"faiss_index_safe\")\n",
    "metadata_dir = os.path.abspath(\"metadata_safe\")\n",
    "os.makedirs(faiss_dir, exist_ok=True)\n",
    "os.makedirs(metadata_dir, exist_ok=True)\n",
    "\n",
    "print(f\"총 {len(car_list)}개의 자동차보험 PDF 문서 처리 시작...\\n\")\n",
    "\n",
    "for file in tqdm(car_list, desc=\"자동차 PDF 처리 중\", unit=\"파일\"):\n",
    "    file_path = os.path.join(base_pdf_dir, file)\n",
    "    base_filename = os.path.splitext(file)[0]\n",
    "    safe_filename = get_safe_filename(base_filename)\n",
    "\n",
    "    print(f\"파일 처리 중: {file} → 저장 파일명: {safe_filename}.faiss\")\n",
    "\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except Exception as e:\n",
    "        print(f\"[오류] {file} 로드 실패: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 텍스트 정제 및 문서화\n",
    "    all_texts = [clean_text(doc.page_content) for doc in documents]\n",
    "    total_text = \"\\n\".join(all_texts)\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))\n",
    "\n",
    "    new_documents = [\n",
    "        Document(page_content=text, metadata={\"page\": i + 1, \"source\": file})\n",
    "        for i, text in enumerate(unique_texts) if text.strip()\n",
    "    ]\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)\n",
    "\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "    embeddings = []\n",
    "    batch_size = 16\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        try:\n",
    "            emb = get_embeddings(batch)\n",
    "            embeddings.append(emb)\n",
    "        except Exception as e:\n",
    "            print(f\"[임베딩 오류] {file} - 배치 {i // batch_size + 1}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not embeddings:\n",
    "        print(f\"[건너뜀] {file} → 유효한 임베딩 없음\")\n",
    "        continue\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "    # FAISS 인덱스 저장\n",
    "    try:\n",
    "        dim = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "        index.add(embeddings)\n",
    "\n",
    "        faiss_path = os.path.join(faiss_dir, f\"{safe_filename}.faiss\")\n",
    "        faiss.write_index(index, faiss_path)\n",
    "        print(f\"{file} → FAISS 저장 완료: {faiss_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAISS 저장 실패] {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 메타데이터 저장\n",
    "    try:\n",
    "        metadata_path = os.path.join(metadata_dir, f\"{safe_filename}.pkl\")\n",
    "        with open(metadata_path, \"wb\") as f:\n",
    "            pickle.dump(split_documents, f)\n",
    "        print(f\"{file} → 메타데이터 저장 완료: {metadata_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[메타데이터 저장 실패] {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n✅ 모든 PDF 문서 처리 및 벡터 저장이 완료되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AXA faiss DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 12개의 자동차보험 PDF 문서 처리 시작...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:   0%|          | 0/12 [00:00<?, ?파일/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 처리 중: 001_개인용자동차보험.pdf → 저장 파일명: doc_08a2926e.faiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:   8%|▊         | 1/12 [1:32:28<16:57:12, 5548.37s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001_개인용자동차보험.pdf → FAISS 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\faiss_index_safe\\doc_08a2926e.faiss\n",
      "001_개인용자동차보험.pdf → 메타데이터 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\metadata_safe\\doc_08a2926e.pkl\n",
      "파일 처리 중: 002_개인용자동차보험공동.pdf → 저장 파일명: doc_5332a74d.faiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  17%|█▋        | 2/12 [2:35:28<12:31:22, 4508.29s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002_개인용자동차보험공동.pdf → FAISS 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\faiss_index_safe\\doc_5332a74d.faiss\n",
      "002_개인용자동차보험공동.pdf → 메타데이터 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\metadata_safe\\doc_5332a74d.pkl\n",
      "파일 처리 중: 003_다이렉트개인용자동차보험.pdf → 저장 파일명: doc_66d02c37.faiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  25%|██▌       | 3/12 [4:12:14<12:45:06, 5100.76s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "003_다이렉트개인용자동차보험.pdf → FAISS 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\faiss_index_safe\\doc_66d02c37.faiss\n",
      "003_다이렉트개인용자동차보험.pdf → 메타데이터 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\metadata_safe\\doc_66d02c37.pkl\n",
      "파일 처리 중: 004_다이렉트개인용인터넷자동차보험.pdf → 저장 파일명: doc_7bb2368f.faiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  33%|███▎      | 4/12 [5:47:56<11:53:50, 5353.79s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "004_다이렉트개인용인터넷자동차보험.pdf → FAISS 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\faiss_index_safe\\doc_7bb2368f.faiss\n",
      "004_다이렉트개인용인터넷자동차보험.pdf → 메타데이터 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\metadata_safe\\doc_7bb2368f.pkl\n",
      "파일 처리 중: 005_다이렉트업무용자동차보험.pdf → 저장 파일명: doc_7d60f594.faiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  42%|████▏     | 5/12 [7:03:08<9:49:11, 5050.21s/파일] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "005_다이렉트업무용자동차보험.pdf → FAISS 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\faiss_index_safe\\doc_7d60f594.faiss\n",
      "005_다이렉트업무용자동차보험.pdf → 메타데이터 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\metadata_safe\\doc_7d60f594.pkl\n",
      "파일 처리 중: 006_다이렉트업무용인터넷자동차보험.pdf → 저장 파일명: doc_810039a3.faiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  50%|█████     | 6/12 [8:18:23<8:06:51, 4868.54s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "006_다이렉트업무용인터넷자동차보험.pdf → FAISS 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\faiss_index_safe\\doc_810039a3.faiss\n",
      "006_다이렉트업무용인터넷자동차보험.pdf → 메타데이터 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\metadata_safe\\doc_810039a3.pkl\n",
      "파일 처리 중: 007_업무용자동차보험.pdf → 저장 파일명: doc_a3166af9.faiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  58%|█████▊    | 7/12 [9:34:24<6:37:18, 4767.72s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "007_업무용자동차보험.pdf → FAISS 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\faiss_index_safe\\doc_a3166af9.faiss\n",
      "007_업무용자동차보험.pdf → 메타데이터 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\metadata_safe\\doc_a3166af9.pkl\n",
      "파일 처리 중: 008_업무용자동차보험공동.pdf → 저장 파일명: doc_f2d10956.faiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  67%|██████▋   | 8/12 [10:34:41<4:53:25, 4401.41s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "008_업무용자동차보험공동.pdf → FAISS 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\faiss_index_safe\\doc_f2d10956.faiss\n",
      "008_업무용자동차보험공동.pdf → 메타데이터 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\metadata_safe\\doc_f2d10956.pkl\n",
      "파일 처리 중: 009_영업용자동차보험.pdf → 저장 파일명: doc_7c397288.faiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  75%|███████▌  | 9/12 [11:35:35<3:28:23, 4167.96s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "009_영업용자동차보험.pdf → FAISS 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\faiss_index_safe\\doc_7c397288.faiss\n",
      "009_영업용자동차보험.pdf → 메타데이터 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\metadata_safe\\doc_7c397288.pkl\n",
      "파일 처리 중: 010_영업용자동차보험공동.pdf → 저장 파일명: doc_fba0b0a0.faiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  83%|████████▎ | 10/12 [12:34:10<2:12:12, 3966.31s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010_영업용자동차보험공동.pdf → FAISS 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\faiss_index_safe\\doc_fba0b0a0.faiss\n",
      "010_영업용자동차보험공동.pdf → 메타데이터 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\metadata_safe\\doc_fba0b0a0.pkl\n",
      "파일 처리 중: 011_이륜자동차보험.pdf → 저장 파일명: doc_0a72f811.faiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중:  92%|█████████▏| 11/12 [13:27:00<1:02:02, 3722.50s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "011_이륜자동차보험.pdf → FAISS 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\faiss_index_safe\\doc_0a72f811.faiss\n",
      "011_이륜자동차보험.pdf → 메타데이터 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\metadata_safe\\doc_0a72f811.pkl\n",
      "파일 처리 중: 012_이륜자동차보험공동.pdf → 저장 파일명: doc_f42fcf39.faiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "자동차 PDF 처리 중: 100%|██████████| 12/12 [14:22:32<00:00, 4312.71s/파일]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "012_이륜자동차보험공동.pdf → FAISS 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\faiss_index_safe\\doc_f42fcf39.faiss\n",
      "012_이륜자동차보험공동.pdf → 메타데이터 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\metadata_safe\\doc_f42fcf39.pkl\n",
      "\n",
      "✅ 파일 매핑 저장 완료: c:\\Users\\Playdata\\OneDrive\\Desktop\\3rd-test\\faiss_index_safe\\file_mapping.json\n",
      "\n",
      "✅ 모든 PDF 문서 처리 및 벡터 저장이 완료되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import hashlib\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 모델 로드\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# 완전한 ASCII-only safe 파일명 생성\n",
    "def get_safe_filename(name):\n",
    "    return \"doc_\" + hashlib.md5(name.encode(\"utf-8\")).hexdigest()[:8]\n",
    "\n",
    "# 임베딩 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    pool_mask = torch.ones_like(inputs[\"input_ids\"]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# 경로 설정\n",
    "base_pdf_dir = \"./pdf_folder_axa/1_자동차보험\"\n",
    "car_list = os.listdir(base_pdf_dir)\n",
    "\n",
    "# 저장 디렉토리\n",
    "faiss_dir = os.path.abspath(\"faiss_index_axa\")\n",
    "metadata_dir = os.path.abspath(\"metadata_axa\")\n",
    "os.makedirs(faiss_dir, exist_ok=True)\n",
    "os.makedirs(metadata_dir, exist_ok=True)\n",
    "\n",
    "# 매핑 정보 저장용\n",
    "file_mapping = {}\n",
    "\n",
    "print(f\"총 {len(car_list)}개의 자동차보험 PDF 문서 처리 시작...\\n\")\n",
    "\n",
    "for file in tqdm(car_list, desc=\"자동차 PDF 처리 중\", unit=\"파일\"):\n",
    "    file_path = os.path.join(base_pdf_dir, file)\n",
    "    base_filename = os.path.splitext(file)[0]\n",
    "    safe_filename = get_safe_filename(base_filename)\n",
    "\n",
    "    file_mapping[safe_filename] = base_filename  # 매핑 저장\n",
    "\n",
    "    print(f\"파일 처리 중: {file} → 저장 파일명: {safe_filename}.faiss\")\n",
    "\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except Exception as e:\n",
    "        print(f\"[오류] {file} 로드 실패: {e}\")\n",
    "        continue\n",
    "\n",
    "    all_texts = [clean_text(doc.page_content) for doc in documents]\n",
    "    total_text = \"\\n\".join(all_texts)\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))\n",
    "\n",
    "    new_documents = [\n",
    "        Document(page_content=text, metadata={\"page\": i + 1, \"source\": file})\n",
    "        for i, text in enumerate(unique_texts) if text.strip()\n",
    "    ]\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)\n",
    "\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "    embeddings = []\n",
    "    batch_size = 16\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        try:\n",
    "            emb = get_embeddings(batch)\n",
    "            embeddings.append(emb)\n",
    "        except Exception as e:\n",
    "            print(f\"[임베딩 오류] {file} - 배치 {i // batch_size + 1}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not embeddings:\n",
    "        print(f\"[건너뜀] {file} → 유효한 임베딩 없음\")\n",
    "        continue\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "    # FAISS 저장\n",
    "    try:\n",
    "        dim = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "        index.add(embeddings)\n",
    "\n",
    "        faiss_path = os.path.join(faiss_dir, f\"{safe_filename}.faiss\")\n",
    "        faiss.write_index(index, faiss_path)\n",
    "        print(f\"{file} → FAISS 저장 완료: {faiss_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAISS 저장 실패] {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 메타데이터 저장\n",
    "    try:\n",
    "        metadata_path = os.path.join(metadata_dir, f\"{safe_filename}.pkl\")\n",
    "        with open(metadata_path, \"wb\") as f:\n",
    "            pickle.dump(split_documents, f)\n",
    "        print(f\"{file} → 메타데이터 저장 완료: {metadata_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[메타데이터 저장 실패] {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "# 매핑 저장\n",
    "mapping_path = os.path.join(faiss_dir, \"file_mapping.json\")\n",
    "with open(mapping_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(file_mapping, f, ensure_ascii=False, indent=2)\n",
    "print(f\"\\n✅ 파일 매핑 저장 완료: {mapping_path}\")\n",
    "\n",
    "print(\"\\n✅ 모든 PDF 문서 처리 및 벡터 저장이 완료되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일반보험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7030fdcd5746e99bed4285010ee21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb63d38d46fa4393836a4c3ffcb7fff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1eedcf2f5a4bf8b716914eb9857601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7080a5a1c868489b851cc9c8371aac5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb4d4951f7e4d16ae58e37e2872a994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_kanana2vec.py:   0%|          | 0.00/10.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding:\n",
      "- configuration_kanana2vec.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7932f7a5d5f3406ebe3116791c8add79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_kanana2vec.py:   0%|          | 0.00/9.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding:\n",
      "- modeling_kanana2vec.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a264082334f4e9bbe35a79a5d489075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 총 12개의 일반보험 PDF 문서 처리 시작...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:   8%|▊         | 1/12 [09:54<1:48:58, 594.39s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 01420250201.pdf 처리 완료 — 462개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  17%|█▋        | 2/12 [10:52<46:26, 278.66s/파일]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 01820250201.pdf 처리 완료 — 478개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  25%|██▌       | 3/12 [11:36<25:45, 171.75s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 01320250201.pdf 처리 완료 — 355개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  33%|███▎      | 4/12 [12:21<16:12, 121.61s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 02120250101.pdf 처리 완료 — 353개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  42%|████▏     | 5/12 [13:12<11:13, 96.29s/파일] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 01520250101.pdf 처리 완료 — 405개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  50%|█████     | 6/12 [14:13<08:25, 84.31s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 01620250101.pdf 처리 완료 — 482개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  58%|█████▊    | 7/12 [15:11<06:18, 75.75s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 02420250101.pdf 처리 완료 — 456개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  67%|██████▋   | 8/12 [16:13<04:44, 71.25s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 01720250201.pdf 처리 완료 — 491개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  75%|███████▌  | 9/12 [16:51<03:02, 60.96s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 02320250201.pdf 처리 완료 — 314개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  83%|████████▎ | 10/12 [17:33<01:50, 55.04s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 01920250101.pdf 처리 완료 — 341개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  92%|█████████▏| 11/12 [17:57<00:45, 45.40s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 02220220101.pdf 처리 완료 — 190개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중: 100%|██████████| 12/12 [18:30<00:00, 92.56s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 02020250201.pdf 처리 완료 — 277개 문서 생성\n",
      "\n",
      "📦 FAISS 인덱스 저장 완료: ./faiss_index_axa_short.bin\n",
      "📝 문서 메타데이터 저장 완료: ./documents_axa_short.pkl\n",
      "\n",
      "🎉 모든 문서 처리 및 저장 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 1️⃣ 모델 및 토크나이저 로드\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 2️⃣ 안전한 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    valid_texts = [t for t in texts if isinstance(t, str) and t.strip()]\n",
    "    if not valid_texts:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        inputs = tokenizer(\n",
    "            valid_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "        return outputs.embedding.cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"[임베딩 오류] 배치 토크나이징 중 예외 발생: {e}\")\n",
    "        return None\n",
    "\n",
    "# 3️⃣ 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# 4️⃣ 파일 경로 설정\n",
    "path = \"pdf_folder_axa\"\n",
    "short_list = os.listdir(os.path.join(path, \"2_일반보험\"))\n",
    "\n",
    "# 5️⃣ 저장 경로\n",
    "faiss_index_path = \"./faiss_index_axa_short.bin\"\n",
    "metadata_path = \"./documents_axa_short.pkl\"\n",
    "\n",
    "# 6️⃣ FAISS GPU 자원 할당 (저장 전에 CPU로 변환할 예정)\n",
    "res = faiss.StandardGpuResources()\n",
    "index = None\n",
    "all_documents = []\n",
    "\n",
    "print(f\"📄 총 {len(short_list)}개의 일반보험 PDF 문서 처리 시작...\\n\")\n",
    "\n",
    "for file in tqdm(short_list, desc=\"PDF 처리 중\", unit=\"파일\"):\n",
    "    all_texts = []\n",
    "    file_path = os.path.join(path, \"2_일반보험\", file)\n",
    "\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except Exception as e:\n",
    "        print(f\"[오류] {file} 로딩 중 문제 발생: {e}\")\n",
    "        continue\n",
    "\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)\n",
    "\n",
    "    total_text = \"\\n\".join(all_texts)\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))\n",
    "\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)\n",
    "\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "\n",
    "        if batch_embeddings is not None:\n",
    "            embeddings.append(batch_embeddings)\n",
    "        else:\n",
    "            print(f\"[스킵] {file} - {i}번째 배치 임베딩 실패\")\n",
    "\n",
    "    if not embeddings:\n",
    "        print(f\"[스킵] {file} - 유효한 임베딩 없음\")\n",
    "        continue\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)\n",
    "        id_index = faiss.IndexIDMap(cpu_index)\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, id_index)  # GPU로 올림\n",
    "\n",
    "    start_id = index.ntotal\n",
    "    ids = np.arange(start_id, start_id + embeddings.shape[0]).astype(np.int64)\n",
    "    index.add_with_ids(embeddings, ids)\n",
    "\n",
    "    all_documents.extend(split_documents)\n",
    "    print(f\"✅ {file} 처리 완료 — {len(split_documents)}개 문서 생성\")\n",
    "\n",
    "# 7️⃣ FAISS 인덱스 저장 (GPU → CPU 변환 후 저장)\n",
    "cpu_index = faiss.index_gpu_to_cpu(index)\n",
    "faiss.write_index(cpu_index, faiss_index_path)\n",
    "print(f\"\\n📦 FAISS 인덱스 저장 완료: {faiss_index_path}\")\n",
    "\n",
    "# 8️⃣ 문서 메타데이터 저장\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "print(f\"📝 문서 메타데이터 저장 완료: {metadata_path}\")\n",
    "\n",
    "print(\"\\n🎉 모든 문서 처리 및 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 장기보험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc3235a4071472e86cc91a82197d10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_kanana2vec.py:   0%|          | 0.00/10.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding:\n",
      "- configuration_kanana2vec.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cb384dd388446cbf0283f3adc85437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_kanana2vec.py:   0%|          | 0.00/9.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding:\n",
      "- modeling_kanana2vec.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf455f2c57d94dfb9df11f40b00e2814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 총 15개의 장기보험 PDF 문서 처리 시작...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:   7%|▋         | 1/15 [02:59<41:59, 179.98s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 03620250101.pdf 처리 완료 — 10885개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  13%|█▎        | 2/15 [03:50<22:30, 103.87s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 02920250101.pdf 처리 완료 — 2788개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  20%|██        | 3/15 [05:13<18:49, 94.10s/파일] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 03320250101.pdf 처리 완료 — 4940개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  27%|██▋       | 4/15 [07:13<19:07, 104.32s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 03220250101.pdf 처리 완료 — 7633개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  33%|███▎      | 5/15 [08:33<15:56, 95.67s/파일] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 03720250101.pdf 처리 완료 — 3962개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  40%|████      | 6/15 [09:22<11:59, 79.91s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 03920250101.pdf 처리 완료 — 2907개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  47%|████▋     | 7/15 [11:48<13:31, 101.43s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 03420250101.pdf 처리 완료 — 9126개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  53%|█████▎    | 8/15 [12:29<09:36, 82.30s/파일] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 03520250101.pdf 처리 완료 — 477개 문서 생성\n",
      "[임베딩 오류] 배치 토크나이징 중 예외 발생: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "[스킵] 03120250101.pdf - 432번째 배치 임베딩 실패\n",
      "[임베딩 오류] 배치 토크나이징 중 예외 발생: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "[스킵] 03120250101.pdf - 448번째 배치 임베딩 실패\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  60%|██████    | 9/15 [14:28<09:21, 93.55s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 03120250101.pdf 처리 완료 — 7597개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  67%|██████▋   | 10/15 [16:31<08:33, 102.73s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 02720250101.pdf 처리 완료 — 6844개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  73%|███████▎  | 11/15 [17:20<05:45, 86.38s/파일] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 03820250101.pdf 처리 완료 — 3020개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  80%|████████  | 12/15 [18:14<03:49, 76.49s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 03020250101.pdf 처리 완료 — 3744개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  87%|████████▋ | 13/15 [19:08<02:19, 69.78s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 02820250101.pdf 처리 완료 — 3733개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중:  93%|█████████▎| 14/15 [21:24<01:29, 89.58s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 02620250101.pdf 처리 완료 — 7661개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF 처리 중: 100%|██████████| 15/15 [21:24<00:00, 85.64s/파일]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 02520111001.pdf 처리 완료 — 3개 문서 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 FAISS 인덱스 저장 완료: ./faiss_index_axa_long.bin\n",
      "📝 문서 메타데이터 저장 완료: ./documents_axa_long.pkl\n",
      "\n",
      "🎉 모든 문서 처리 및 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 1️⃣ 모델 및 토크나이저 로드\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 2️⃣ 안전한 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    valid_texts = [t for t in texts if isinstance(t, str) and t.strip()]\n",
    "    if not valid_texts:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        inputs = tokenizer(\n",
    "            valid_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "        return outputs.embedding.cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"[임베딩 오류] 배치 토크나이징 중 예외 발생: {e}\")\n",
    "        return None\n",
    "\n",
    "# 3️⃣ 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# 4️⃣ 파일 경로 설정\n",
    "path = \"pdf_folder_axa\"\n",
    "car_list = os.listdir(os.path.join(path, \"1_자동차보험\"))\n",
    "short_list = os.listdir(os.path.join(path, \"2_일반보험\"))\n",
    "long_list = os.listdir(os.path.join(path, \"3_장기보험\"))\n",
    "\n",
    "long_hurt = []\n",
    "long_etc = []\n",
    "\n",
    "\n",
    "# 5️⃣ 저장 경로\n",
    "faiss_index_path = \"./faiss_index_axa_long.bin\"\n",
    "metadata_path = \"./documents_axa_long.pkl\"\n",
    "\n",
    "# 6️⃣ FAISS 인덱스 준비 (저장 가능하도록 IndexIDMap 사용)\n",
    "index = None\n",
    "all_documents = []\n",
    "\n",
    "print(f\"📄 총 {len(long_list)}개의 장기보험 PDF 문서 처리 시작...\\n\")\n",
    "\n",
    "for file in tqdm(long_list, desc=\"PDF 처리 중\", unit=\"파일\"):\n",
    "    all_texts = []\n",
    "    file_path = os.path.join(path, \"3_장기보험\", file)\n",
    "\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except Exception as e:\n",
    "        print(f\"[오류] {file} 로딩 중 문제 발생: {e}\")\n",
    "        continue\n",
    "\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)\n",
    "\n",
    "    total_text = \"\\n\".join(all_texts)\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))\n",
    "\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)\n",
    "\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "\n",
    "        if batch_embeddings is not None:\n",
    "            embeddings.append(batch_embeddings)\n",
    "        else:\n",
    "            print(f\"[스킵] {file} - {i}번째 배치 임베딩 실패\")\n",
    "\n",
    "    if not embeddings:\n",
    "        print(f\"[스킵] {file} - 유효한 임베딩 없음\")\n",
    "        continue\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        base_index = faiss.IndexFlatL2(embedding_dim)\n",
    "        index = faiss.IndexIDMap(base_index)\n",
    "\n",
    "    start_id = index.ntotal\n",
    "    ids = np.arange(start_id, start_id + embeddings.shape[0]).astype(np.int64)\n",
    "    index.add_with_ids(embeddings, ids)\n",
    "\n",
    "    all_documents.extend(split_documents)\n",
    "    print(f\"✅ {file} 처리 완료 — {len(split_documents)}개 문서 생성\")\n",
    "\n",
    "# 7️⃣ FAISS 인덱스 저장\n",
    "faiss.write_index(index, faiss_index_path)\n",
    "print(f\"\\n📦 FAISS 인덱스 저장 완료: {faiss_index_path}\")\n",
    "\n",
    "# 8️⃣ 문서 메타데이터 저장\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "print(f\"📝 문서 메타데이터 저장 완료: {metadata_path}\")\n",
    "\n",
    "print(\"\\n🎉 모든 문서 처리 및 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ 저장된 FAISS 인덱스 로드\n",
    "index = faiss.read_index(\"./faiss_index.bin\")\n",
    "\n",
    "# 2️⃣ 문서 정보 로드\n",
    "with open(\"./documents.pkl\", \"rb\") as f:\n",
    "    documents = pickle.load(f)\n",
    "\n",
    "# 3️⃣ 검색 수행\n",
    "# query = \" 보험금의 종류 및 한도에 대해 설명해줘?\"\n",
    "query = \"자동차 의무보험 미가입에 따른 불이익을 알려줘\"\n",
    "query_embedding = get_embeddings([query])[0]  # 쿼리 임베딩\n",
    "query_embedding = np.array([query_embedding], dtype=np.float32)\n",
    "\n",
    "D, I = index.search(query_embedding, k=10)  # 가장 유사한 5개 검색\n",
    "context = []\n",
    "\n",
    "# 4️⃣ 검색 결과 출력\n",
    "for idx in I[0]:\n",
    "    context.append(documents[idx].page_content)\n",
    "    print(f\"🔹 문서 {idx}: {documents[idx].page_content[:200]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectordb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
