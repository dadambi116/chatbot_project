import os
import json
import pickle
import faiss

def build_vector_db_sources(companies, categories):
    """
    ë³´í—˜ì‚¬ + ë³´í—˜ì¢…ë¥˜ ì¡°í•©ì— ë”°ë¼ ë²¡í„°DB ë””ë ‰í† ë¦¬ ì •ë³´ë¥¼ êµ¬ì„±
    ì˜ˆ: AXA + car â†’ ./AXA_db/faiss_index_axa_car, metadata_axa_car, file_mapping_axa_car.json
    """
    base_dir = "llm/vector_db"
    sources = []

    for company in companies:
        for category in categories:
            company_key = f"{company.upper()}_db"
            index_dir = os.path.join(base_dir, company_key, f"faiss_index_{company.lower()}_{category}")
            meta_dir = os.path.join(base_dir, company_key, f"metadata_{company.lower()}_{category}")
            mapping_path = os.path.join(base_dir, company_key, f"file_mapping_{company.lower()}_{category}.json")

            if os.path.exists(index_dir) and os.path.exists(meta_dir) and os.path.exists(mapping_path):
                sources.append({
                    "label": company,
                    "category": category,
                    "index_dir": index_dir,
                    "meta_dir": meta_dir,
                    "mapping_path": mapping_path
                })
            # print(os.path.exists(index_dir))
    return sources

def search_all_sources(query, sources, embedder, top_k=5):
    query_vector = embedder.get_embedding(query)
    all_results = []

    for source in sources:
        INDEX_DIR = source["index_dir"]
        META_DIR = source["meta_dir"]
        MAPPING_PATH = source["mapping_path"]
        company = source["label"]
        category = source["category"]

        # ìƒí’ˆëª… ë§¤í•‘ ë¡œë“œ
        if not os.path.exists(MAPPING_PATH):
            continue
        with open(MAPPING_PATH, "r", encoding="utf-8") as f:
            file_mapping = json.load(f)

        # í´ë” ë‚´ ëª¨ë“  ìƒí’ˆë³„ faiss/pkl ê²€ìƒ‰
        for file in os.listdir(INDEX_DIR):
            if not file.endswith(".faiss"):
                continue

            doc_id = file.replace(".faiss", "")
            faiss_path = os.path.join(INDEX_DIR, file)
            meta_path = os.path.join(META_DIR, f"{doc_id}.pkl")

            if not os.path.exists(faiss_path) or not os.path.exists(meta_path):
                continue

            index = faiss.read_index(faiss_path)
            with open(meta_path, "rb") as f:
                passages = pickle.load(f)

            D, I = index.search(query_vector, top_k * 3)  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ë„‰ë„‰íˆ ì¶”ì¶œ
            for dist, idx in zip(D[0], I[0]):
                if idx < len(passages):
                    text = passages[idx]
                    all_results.append({
                        "text": str(text.page_content if hasattr(text, "page_content") else text),
                        "distance": float(dist),
                        "doc_id": doc_id,
                        "doc_name": file_mapping.get(doc_id, doc_id),
                        "company": company,
                        "category": category
                    })
    print("[ğŸ” ë²¡í„° ê²€ìƒ‰ ê²°ê³¼]")
    for r in all_results[:5]:  # ìƒìœ„ 5ê°œë§Œ
        print(f"- ê±°ë¦¬: {r['distance']:.4f} | ë³´í—˜ì‚¬: {r['company']} | ì¹´í…Œê³ ë¦¬: {r['category']}")
        print(f"  í…ìŠ¤íŠ¸: {r['text'][:80]}...\n")
        
    # ì¤‘ë³µ ì œê±° + top_k ì •ë ¬
    seen = set()
    final_results = []
    for result in sorted(all_results, key=lambda x: x["distance"]):
        norm = " ".join(result["text"].strip().lower().split())
        if norm not in seen:
            seen.add(norm)
            final_results.append(result)
        if len(final_results) >= top_k:
            break

    
    return final_results

