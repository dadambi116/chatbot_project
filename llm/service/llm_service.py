# llm_service.py
from llm.model.generator import Generator
from llm.service.search_service import SearchService

class LLMService:
    def __init__(self):
        self.generator = Generator("CarrotAI/Llama-3.2-Rabbit-Ko-3B-Instruct")
        self.search_service = SearchService()

    def generate_answer_with_filtering(self, question, companies, categories):
        context_docs = self.search_service.search_with_rerank(question, companies, categories)

        if not context_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        context = "\n".join([doc['text'] for doc in context_docs])
        prompt = self._build_prompt(context, question)
        print("\n[ğŸ“¥ LLM ì…ë ¥ í”„ë¡¬í”„íŠ¸]")
        print(prompt[:300])  # ë„ˆë¬´ ê¸¸ë©´ 300ìê¹Œì§€ë§Œ

        answer = self.generator.generate(prompt)
        print("\n[ğŸ¤– LLM ìƒì„± ì‘ë‹µ]")
        print(answer)
        return answer

    def _build_prompt(self, context, question):
        return f"[ë¬¸ì„œ ì‹œì‘]\n{context}\n[ë¬¸ì„œ ë]\n\n[ì§ˆë¬¸]\n{question}\n\n[ë‹µë³€]"
